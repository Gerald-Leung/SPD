{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan for New SPD Files\n",
    "Two approaches have been attempted: By reading the HTML content using bs4 and \"hashing\" the content using hashlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.request import urlopen, Request\n",
    "import filecmp\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import wget\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and reading the URL\n",
    "We first input the desired url and parse the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = Request('https://www.nrscotland.gov.uk/statistics-and-data/geography/nrs-postcode-extract',\n",
    "              headers={'User-Agent': 'Moziilla/5.0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the url and convert to a \"soup\" object to read the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urlopen(url).read()\n",
    "soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for the target location\n",
    "Now we search for the required information, which is the '2021-2' link to the latest SPD files. This is subject to change in the future, e.g., when it is updated to '2022-2'. Assuming the format of the names remains unchanged, we can simply replace the '2021-2' below with the next expected name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various methods to look for our target information. From above, upon going through the HTML content, we find that the tags 'td' (for tables) and 'a' (for hyperlinks) are associated with our target. In fact, we know this by just examining the website since the link to '2021-2' is part of a table, and is a hyperlink (this is more obvious). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the find_all function to look for content associated with the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td valign=\"top\" width=\"15%\"><strong>Latest</strong></td>,\n",
       " <td valign=\"top\" width=\"85%\"><a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a></td>,\n",
       " <td valign=\"top\" width=\"15%\"><strong>Previous</strong></td>,\n",
       " <td valign=\"top\" width=\"85%\"><a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-1\">2021-1</a></td>,\n",
       " <td valign=\"top\" width=\"15%\"><strong>Archive</strong></td>,\n",
       " <td valign=\"top\" width=\"85%\"><a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/archived-postcode-extract\">2012-2 onwards</a></td>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"td\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the results, we see that the desired result is the second item. So we extract it and turn it into a string to confirm this is indeed what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_label = soup.find_all(\"td\")[1]\n",
    "section_label.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then search for hyperlinks within the website. There are *many* hyperlinks on the website so we get a long list of results. Specifically, by inspecting the soup object, we find that the required link is located on line 36 (this is rather dumb, a smarter way is shown further down below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\")[36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above method can be difficult since we have the spot this specific link manually from a list of messy results (websites are often very complicated). We know that the name of the link is '2021-2' so we attempt to find the link with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>,\n",
       " <a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a',text='2021-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef search_link(response_object,tag,date):\\n    k = BeautifulSoup(response_object,\"html.parser\")\\n    a = k.find_all(\\'{}\\'.format(tag),text=\\'{}\\'.format(date))\\n    #a = k.find_all(\\'{0},text={1}\\'.format(tag,date))\\n    #b = k.find_all(\\'{0}\\'.format(tag))\\n    return a\\n\\na = \"a\"\\nb = \"2021-2\"\\nx = search_link(response,a,b)\\nx\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def search_link(response_object,tag,date):\n",
    "    k = BeautifulSoup(response_object,\"html.parser\")\n",
    "    a = k.find_all('{}'.format(tag),text='{}'.format(date))\n",
    "    #a = k.find_all('{0},text={1}'.format(tag,date))\n",
    "    #b = k.find_all('{0}'.format(tag))\n",
    "    return a\n",
    "\n",
    "a = \"a\"\n",
    "b = \"2021-2\"\n",
    "x = search_link(response,a,b)\n",
    "x\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two links on the url which are identical so we pick the latter one and extract the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1=soup.find_all(\"a\",text='2021-2')[1]\n",
    "t1=target1.string\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to directly look for the exact link. But this requires a precise link that must be predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_link = soup.find_all(\"a\",href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\")[1]\n",
    "target_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlink = soup.find_all(\"a\")[36]\n",
    "newlinkstr = str(newlink) # turn content into a string\n",
    "#dllink = newlink['href']\n",
    "#newlink.string # gives same result as section_label.string\n",
    "newlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also \"hash\" the entire webpage and compare the content later on to spot changes. However, this is not ideal as it can detect very subtle changes. \n",
    "\n",
    "We can try to isolate a certain part of the website and apply this method. This requires a bit more thinking and we should come back to this in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newHash = hashlib.sha224(response).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for updates\n",
    "Now we want to know if there has been any updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the above results into a text file (here we use the HTML results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('currentSPD.txt','w') as f:\n",
    "    f.write(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare the content of the textfiles to see if there has been any changes. Result will be true if there are no updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = \"./currentSPD.txt\"\n",
    "f2 = \"./oldSPD.txt\"\n",
    "\n",
    "result = filecmp.cmp(f1, f2, shallow=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write the current HTML content into \"oldSPD.txt\" so that it can be used for comparison next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the website for updates.\n"
     ]
    }
   ],
   "source": [
    "if result == True:\n",
    "    print('No new SPD files.')\n",
    "else:\n",
    "    print('Check the website for updates.')\n",
    "    with open('oldSPD.txt','w') as f:\n",
    "        f.write(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the target link\n",
    "Now that we located the desired link, we shall access it to move to the next page and download our target file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's examine our result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2\">2021-2</a>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the link contained in there is missing the base url. However, we know what it is by examining the website ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_target = \"https://www.nrscotland.gov.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the actual url and parse it. But first let's extract the \"href\" content from our result above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_linkhref = target_link[\"href\"]\n",
    "target_linkhref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nrscotland.gov.uk/statistics-and-data/geography/our-products/scottish-postcode-directory/2021-2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_url = base_target+target_linkhref\n",
    "target_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly what we need! Now we read and parse the url as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target_url = Request(target_url,headers={'User-Agent': 'Moziilla/5.0'})\n",
    "target_html = urlopen(get_target_url).read()\n",
    "#target_text = target_html.read().decode(\"utf-8\")\n",
    "#target_url_response = target_html.read()\n",
    "target_soup = BeautifulSoup(target_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a beautiful soup object again. Similar to before, we look for our target file: SPD_PostcoodeIndex_Cut_21_2_csv.zip. Again, this is only possible when we can predict the file names and names of the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/files//statistics/geography/2021-2/SPD_PostcodeIndex_Cut_21_2_Access.zip\">Postcode Index</a>,\n",
       " <a href=\"/files//statistics/geography/2021-2/SPD_PostcodeIndex_Cut_21_2_CSV.zip\">Postcode Index</a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_soup.find_all(\"a\",text='Postcode Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second result is what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/files//statistics/geography/2021-2/SPD_PostcodeIndex_Cut_21_2_CSV.zip'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_target = target_soup.find_all(\"a\",text='Postcode Index')[1][\"href\"]\n",
    "file_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we construct the full link with the previous base link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nrscotland.gov.uk/files//statistics/geography/2021-2/SPD_PostcodeIndex_Cut_21_2_CSV.zip'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_target = base_target+file_target\n",
    "final_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the files\n",
    "Now we can download the desired files. The content will be saved in the \"SPD_PostcodeIndex_Cut_21_2_CSV\" folder. You might get an error saying the target is not a zip file. This is still being investigated by you might want to make sure you do not have any files that share the same name in your downloads folder.\n",
    "\n",
    "**Edit**: this is probably due to the 403 Forbidden error that we encountered using urllib before. Is it due to certain permission or we are blocked from downloading through Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporary workaround:** run all cells first before running the download cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "r = requests.get(final_target,headers=user_agent)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"./SPD_PostcodeIndex_Cut_21_2_CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do\n",
    "* Currently this notebook only demonstrates how this can be done in general. A Python script is being written to execute the process and will restructure the methods (e.g. if there are no updates, then there is no need to download anything)\n",
    "* Make it work with real time, i.e. scheduling. This will probably require some integration with the command-line\n",
    "* Make the whole process smarter, e.g. as mentioned above, no need to run further if there are no updates, and accept input arguments (e.g. if you want to look for a specific date/file name you can just throw it in from the command-line when you run the script without having to click into the script, scrolling through and editing it manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
